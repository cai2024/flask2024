Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 20
Rules claiming more threads will be scaled down.
Job stats:
job         count
--------  -------
all             1
fastp_pe        1
total           2

Select jobs to execute...
Execute 1 jobs...

[Fri May  3 17:27:34 2024]
localrule fastp_pe:
    input: /data/cailab/temp/fastq/1w_1.fq.gz, /data/cailab/temp/fastq/1w_2.fq.gz
    output: /data/cailab/temp/trimmed/trim_1w_1.fq.gz, /data/cailab/temp/trimmed/trim_1w_2.fq.gz, /data/cailab/temp/trimmed/1w.json, /data/cailab/temp/trimmed/1w.html
    log: /data/cailab/temp/log/fastp/1w.log
    jobid: 1
    reason: Code has changed since last execution
    wildcards: sample=1w
    threads: 2
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=./temp_test

Activating conda environment: flask2024
[Fri May  3 17:27:36 2024]
Finished job 1.
1 of 2 steps (50%) done
Select jobs to execute...
Execute 1 jobs...

[Fri May  3 17:27:36 2024]
localrule all:
    input: /data/cailab/temp/trimmed/trim_1w_1.fq.gz
    jobid: 0
    reason: Input files updated by another job: /data/cailab/temp/trimmed/trim_1w_1.fq.gz
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=./temp_test

[Fri May  3 17:27:36 2024]
Finished job 0.
2 of 2 steps (100%) done
Complete log: .snakemake/log/2024-05-03T172733.020430.snakemake.log
